# Common
import numpy as np
import sys
from matplotlib import pyplot as plt
import scipy as sp
from scipy import stats
from multiprocessing import Process, Value, Array, Pool
import multiprocessing
import time
import pandas as pd

# Specialized
from colossus.lss import mass_function
from colossus.cosmology import cosmology
from Corrfunc.theory import wp

# Local
from AGNCatalogToolbox import Utillity as utl
from AGNCatalogToolbox import ImageGenerator as img


def generate_semi_analytic_halo_catalogue(catalogue_volume,
                                          mass_params,
                                          z,
                                          h,
                                          visual_debugging=False,
                                          erase_debugging_folder=False,
                                          visual_debugging_path="./visualValidation/SemiAnalyticCatalog/"):
    """ Function to generate the semi analytic halo catalogue (without coordinates) for galaxy testing

    :param catalogue_volume: float, cosmological volume within which to generate the catalog.
    :param mass_params: tuple, (mass low, mass high, spacing) in log10.
    :param z: float, redshift.
    :param h: float, reduced hubble constant.
    :param visual_debugging: bool, switch on visual debugging, which plots and outputs the black hole mass function.
    :param erase_debugging_folder: bool, if we should completely erase the contents of the folder we are writing plots
    to in advance of writing our own plots. Be exceptionally careful with this - it will list the files and ask for
    approval. If in doubt, leave False. We include this because old plots can lead to misinformation.
    :param visual_debugging_path: float, the path to which plots will be sent. Be careful where you point this if you
    set erase_debugging folder to True, as it will try and erase the contents.
    :return array, of halo masses (log10).
    """

    # Call funtion to prep for visual debugging.
    utl.visual_debugging_housekeeping(visual_debugging=visual_debugging,
                                  function_name="generate_semi_analytic_halo_catalogue",
                                  erase_debugging_folder=erase_debugging_folder,
                                  visual_debugging_path=visual_debugging_path
                                  )
    # Get the bin width and generate the bins. 
    bin_width = mass_params[2]
    mass_range = 10 ** np.arange(mass_params[0], mass_params[1], mass_params[2]) + np.log10(h)  # h^-1

    # Generate the mass function itself - this is from the colossus toolbox
    local_mass_function = mass_function.massFunction(mass_range, z, mdef='200m', model='tinker08', q_out='dndlnM') \
        * np.log(10) / h  # dn/dlog10M

    # Plot the halo mass function generated by colossus, if we activated visual debugging.
    if visual_debugging:
        fig = plt.figure()
        plt.xlabel('M200m')
        plt.ylabel('M')
        plt.title('Halo Mass function from Colossus')
        plt.loglog()
        plt.plot(mass_range, local_mass_function, '-', label='z = %.1f' % z)
        plt.legend()
        save_path = visual_debugging_path + 'Colossus_HMF.png'
        print("Writing file: {}".format(save_path))
        fig.savefig(save_path)
        plt.close()

    # We determine the Cumulative HMF starting from the high mass end, multiplied by the bin width.
    # This effectively gives the cumulative probability of a halo existing.
    cumulative_mass_function = np.flip(np.cumsum(np.flip(local_mass_function, 0)), 0) * bin_width

    ########################################################################
    # Interpolation Tests
    # Interpolator for the testing - we will update this with the volume in a second.
    # This is essentially for a volume of size unity.
    interpolator = sp.interpolate.interp1d(cumulative_mass_function, mass_range)

    sample_index = int(np.floor(len(cumulative_mass_function) / 2))  # index of the half way point
    num_test = cumulative_mass_function[sample_index]  # The value of the cum function at this index
    mass_test = interpolator(num_test)  # Interpolate to get the mass that this predicts
    # Check that these values are equivalent.
    assert mass_range[sample_index] == mass_test, \
        "Interpolation method incorrect: Back interpolation at midpoint failed"
    # Check first element is equivalent to the total to 10 SF accuracy
    assert np.round(cumulative_mass_function[0], 10) ==\
        np.round(np.sum(local_mass_function) * bin_width, 10), "Final cum sum element != total sum"
    ########################################################################

    # Multiply by volume
    cumulative_mass_function = cumulative_mass_function * catalogue_volume

    # Get the maximum cumulative number.
    max_number = np.floor(cumulative_mass_function[0])
    range_numbers = np.arange(max_number)

    # Update interpolator
    interpolator = sp.interpolate.interp1d(cumulative_mass_function, mass_range)
    mass_catalog = interpolator(range_numbers[range_numbers >= np.amin(cumulative_mass_function)])

    # Reconstruct HMF
    width = 0.1
    bins = np.arange(10, 16, width)
    hist = np.histogram(np.log10(mass_catalog), bins=bins)[0]
    hmf = (hist / catalogue_volume) / width

    # Plot both as a comparison
    if visual_debugging:
        # Plot both as a comparison
        plt.figure()
        plt.loglog()

        bins_power = 10 ** (bins[0:-1])

        plt.plot(bins_power[hmf != 0], hmf[hmf != 0], 'o', label='Reconstructed')
        plt.plot(mass_range, local_mass_function, label='Original')

        plt.legend()
        plt.xlabel("Halo Mass")
        plt.ylabel("Number Density")
        plt.title("Reconstructed HMF")

        save_path = visual_debugging_path + 'HMF_Validation.png'
        print("Writing file: {}".format(save_path))
        plt.savefig(save_path)
        plt.close()

    mass_catalog = np.log10(mass_catalog)
    return mass_catalog


def load_halo_catalog(h, z, cosmology, filename="MD_", path_big_data="./BigData/",
                      user = "Chris",
                      visual_debugging=False,
                      erase_debugging_folder=False,
                      visual_debugging_path="./visualValidation/nbodyCatalog/"):
    """ Function to load in the catalog_data from the multi-dark halo catalogue

    This catalog_data should exist as .npy files in the Directory/BigData. Within
    this folder there should be a script to pull these out of the SQL
    database. Note that this expects a .npy file in the with columns x, y, z
    scale factor at accretion, mass at accretion. If generateFigures is set
    to True (default), then a further column of the halo mass is also
    required to validate the halos.

    :param h: float, reduced hubble constant
    :param z: float, redshift
    :param cosmology: Colossus cosmology object, the cosmology
    :param filename: string, component of the filename excluding the redshift - the closest z will be found
    automatically. Default is "MD_", expecting files of the form "MD_0.0.npy".
    :param path_big_data: string, path to the location of the data.
    :param user: string. This is an adjustable string that allows Viola to use her catalogs quickly. Defaults to "Chris".
    :param visual_debugging: bool, switch on visual debugging, which plots and outputs the stellar mass function.
    :param erase_debugging_folder: bool, if we should completely erase the contents of the folder we are writing plots
    to in advance of writing our own plots. Be exceptionally careful with this - it will list the files and ask for
    approval. If in doubt, leave False. We include this because old plots can lead to misinformation.
    :param visual_debugging_path: float, the path to which plots will be sent. Be careful where you point this if you
    set erase_debugging folder to True, as it will try and erase the contents.
    :return effective_halo_mass (array), effective_z (array), virial_mass (array), up_id (array), x, y, z (arrays).

    """
    utl.visual_debugging_housekeeping(visual_debugging=visual_debugging,
                                  function_name="load_halo_catalog",
                                  erase_debugging_folder=erase_debugging_folder,
                                  visual_debugging_path=visual_debugging_path)

    print("Loading Halo Catalogue")
    volume_axis = 1000/h
    volume = volume_axis**3  # MultiDark Box size, Mpc

    # Search for the closest file and read it in.
    if user == "Viola":
        catalog_z = 0.1
        catalog_data = np.load("MD_1.npy")
    else:
        catalog_file, catalog_z = utl.GetCorrectFile(filename, z, path_big_data, True)
        print("Found file:", catalog_file)
        catalog_data = np.load(path_big_data + catalog_file)
        print("dtypes found: ", catalog_data.dtype)

    # If we want to generate the Halo Mass function figure, this section.
    if visual_debugging:
        PlotHaloMassFunction(catalog_data["mvir"][catalog_data["upid"] == -1] / h, z, volume, cosmology,
                             visual_debugging_path)

    if user == "Viola":
        data_x = catalog_data['x']/h
        data_y = catalog_data['y']/h
        data_z = catalog_data['z']/h
        main_id = catalog_data['rockstarId']
        up_id = catalog_data['upId']
        virial_mass = catalog_data['Mvir']/h
        mass_at_accretion = catalog_data['irst_Acc_MvirF']/h
        accretion_scale = catalog_data['irst_Acc_Scale']
    else:
        data_x = catalog_data['x']#/h
        data_y = catalog_data['y']#/h
        data_z = catalog_data['z']#/h
        main_id = catalog_data["id"]
        up_id = catalog_data["upid"]
        virial_mass = catalog_data["mvir"]/h
        mass_at_accretion = catalog_data["First_Acc_Mvir"]/h
        accretion_scale = catalog_data["First_Acc_Scale"]

    del catalog_data  # Save on memory

    # Reserved memory
    virial_mass_parent = np.zeros_like(virial_mass)
    idh = np.zeros_like(virial_mass)
    effective_z = np.zeros_like(accretion_scale)

    print('    Sorting list w.r.t. upId')
    sorted_indexes = up_id.argsort()  # + 1 Array, rest are reused
    # To maintain order, we update the class data. This only needs to be done once.
    data_x = data_x[sorted_indexes]
    data_y = data_y[sorted_indexes]
    data_z = data_z[sorted_indexes]
    # We also do this with the local data
    main_id = main_id[sorted_indexes]
    up_id = up_id[sorted_indexes]
    virial_mass = virial_mass[sorted_indexes]
    mass_at_accretion = mass_at_accretion[sorted_indexes]
    accretion_scale = accretion_scale[sorted_indexes]
    up_id_0 = np.searchsorted(up_id, 0)  # Position where zero should sit on the now sorted up_id.
    # All arrays are should now sorted by up_id.

    print('    copying all {} elements with up_id = -1'.format(str(up_id_0)))
    virial_mass_parent[:up_id_0] = virial_mass[:up_id_0]  # MVir of centrals, or where up_id  = -1.

    print('    sorting remaining list list w.r.t. main id')
    up_id_cut = up_id[up_id_0:]  # Up_id's that are not -1, or the satellites, value pointing to their progenitor.

    id_cut = main_id[:up_id_0]  # ids of centrals
    virial_mass_cut = virial_mass[:up_id_0]  # masses of centrals

    sorted_indexes = id_cut.argsort()  # get indexes to centrals by id.
    id_cut = id_cut[sorted_indexes]  # actually sort centrals by id.
    virial_mass_cut = virial_mass_cut[sorted_indexes]  # sort virial masses the same way.

    print('    copying remaining', str(len(up_id) - up_id_0), 'elements')
    sorted_indexes = np.searchsorted(id_cut, up_id_cut)  # indexes of where satellite id's point to centrals
    virial_mass_parent[up_id_0:] = virial_mass_cut[sorted_indexes]  # Sort parents by this, and assign to satellites
    # This gives us the virial mass of the parent or itself if it is a parent. But do we actually need this?
    idh[up_id_0:] = 1

    halo_mass = virial_mass
    halo_mass[idh > 0] = mass_at_accretion[idh > 0]

    effective_z[idh > 0] = 1 / accretion_scale[idh > 0] - 1
    effective_z[idh < 1] = catalog_z

    # self.main_catalog['virial_mass'] = virial_mass # Not sure we actually need this?
    effective_halo_mass = np.log10(halo_mass)
    # self.main_catalog['parent_halo_mass'] = np.log10(virial_mass_parent)
    # self.main_catalog['up_id'] = up_id
    return effective_halo_mass, effective_z, np.log10(virial_mass), up_id, data_x, data_y, data_z


def halo_mass_to_stellar_mass(halo_mass,
                              z,
                              formula="Grylls19",
                              scatter=0.11,
                              visual_debugging=False,
                              erase_debugging_folder=False,
                              debugging_volume=500 ** 3,
                              visual_debugging_path="./"):
    """Function to generate stellar masses from halo masses.

    This is based on Grylls 2019, but also has the option to use the
    parameters from Moster. This is a simplified version of Pip's
    DarkMatterToStellarMass() function.

    :param halo_mass: array, of halo masses (log10)
    :param z: float, the value of redshift
    :param formula: string, the method to use. Options currently include "Grylls19" and "Moster"
    :param scatter: bool, to scatter or not
    :param visual_debugging: bool, switch on visual debugging, which plots and outputs the stellar mass function.
    :param erase_debugging_folder: bool, if we should completely erase the contents of the folder we are writing plots
    to in advance of writing our own plots. Be exceptionally careful with this - it will list the files and ask for
    approval. If in doubt, leave False. We include this because old plots can lead to misinformation.
    :param debugging_volume: float, the cosmological volume, for the purposes of calculating the SMF for testing.
    :param visual_debugging_path: float, the path to which plots will be sent. Be careful where you point this if you
    set erase_debugging folder to True, as it will try and erase the contents.
    :return array, of stellar masses (log10).
    """
    utl.visual_debugging_housekeeping(visual_debugging=visual_debugging,
                                  function_name="halo_mass_to_stellar_mass",
                                  erase_debugging_folder=erase_debugging_folder,
                                  visual_debugging_path=visual_debugging_path)

    # If conditions to set the correct parameters.
    if formula == "Grylls19":
        z_parameter = np.divide(z - 0.1, z + 1)
        m_10, shm_norm_10, beta10, gamma10 = 11.95, 0.032, 1.61, 0.54
        m_11, shm_norm_11, beta11, gamma11 = 0.4, -0.02, -0.6, -0.1
    elif formula == "Moster":
        z_parameter = np.divide(z, z + 1)
        m_10, shm_norm_10, beta10, gamma10 = 11.590, 0.0351, 1.376, 0.608
        m_11, shm_norm_11, beta11, gamma11 = 1.195, -0.0247, -0.826, 0.329
    else:
        assert False, "Unrecognised formula"

    # Create full parameters
    m = m_10 + m_11 * z_parameter
    n = shm_norm_10 + shm_norm_11 * z_parameter
    b = beta10 + beta11 * z_parameter
    g = gamma10 + gamma11 * z_parameter
    # Full formula
    internal_stellar_mass = np.log10(np.power(10, halo_mass) *
                                     (2 * n * np.power((np.power(np.power(10, halo_mass - m), -b)
                                                        + np.power(np.power(10, halo_mass - m), g)), -1)))
    # Add scatter, if requested.
    if not scatter == False:
        print("Scatter is a thing, valued at {}".format(scatter))
        internal_stellar_mass += np.random.normal(scale=scatter, size=np.shape(internal_stellar_mass))

    # Generate the figures, if requested.
    if visual_debugging:
        width = 0.1
        bins = np.arange(9, 15, width)

        hist = np.histogram(internal_stellar_mass, bins=bins)[0]
        hmf = (hist / debugging_volume) / width
        log_smf = np.log10(hmf[hmf != 0])
        adj_bins = bins[0:-1][hmf != 0]

        plt.figure()
        plt.loglog()
        plt.plot(10 ** adj_bins, (10 ** log_smf), label="Grylls 2019")
        plt.xlabel("Stellar Mass")
        plt.ylabel("phi")
        plt.title("Stellar Mass Function, assigned from Pip's code")
        plt.legend()
        save_path = visual_debugging_path + 'SMF_Validation.png'
        print("Writing file: {}".format(save_path))
        plt.savefig(save_path)
        plt.close()

    return internal_stellar_mass


def stellar_mass_to_black_hole_mass(stellar_mass,
                                    method="Shankar16",
                                    scatter="Intrinsic",
                                    visual_debugging=False,
                                    erase_debugging_folder=False,
                                    debugging_volume=500 ** 3,
                                    visual_debugging_path="./figures/"):
    """ Function to assign black hole mass from the stellar mass.

    :param stellar_mass: array, of stellar masses in log10
    :param method: string, specifying the method to be used, current options are "Shankar16",  "KormondyHo" and "Eq4".
    :param scatter: string or float, string should be "Intrinsic", float value specifies the (fixed) scatter magnitude
    :param visual_debugging: bool, switch on visual debugging, which plots and outputs the black hole mass function.
    :param erase_debugging_folder: bool, if we should completely erase the contents of the folder we are writing plots
    to in advance of writing our own plots. Be exceptionally careful with this - it will list the files and ask for
    approval. If in doubt, leave False. We include this because old plots can lead to misinformation.
    :param debugging_volume: float, the cosmological volume, for the purposes of calculating the BHMF for testing.
    :param visual_debugging_path: float, the path to which plots will be sent. Be careful where you point this if you
    set erase_debugging folder to True, as it will try and erase the contents.
    :return: array, of the black hole masses (log10).
    """
    utl.visual_debugging_housekeeping(visual_debugging=visual_debugging,
                                  function_name="stellar_mass_to_black_hole_mass",
                                  erase_debugging_folder=erase_debugging_folder,
                                  visual_debugging_path=visual_debugging_path)
    if method == "Shankar16":
        log_black_hole_mass = 7.574 + 1.946 * (stellar_mass - 11) \
                              - 0.306 * (stellar_mass - 11)**2. \
                              - 0.011 * (stellar_mass - 11)**3.
        if scatter == "Intrinsic" or scatter == "intrinsic":
            log_black_hole_mass += (0.32 - 0.1*(stellar_mass - 12.)) * np.random.normal(0., 1., len(stellar_mass))
        elif isinstance(type(scatter), float):
            log_black_hole_mass += np.random.normal(0., scatter, len(stellar_mass))
        elif scatter == False or scatter == None:
            pass
        else:
            assert False, "Unknown Scatter argument {}".format(scatter)

    elif method == "KormondyHo":
        log_black_hole_mass = 8.54 + 1.18 * (stellar_mass - 11)
        if scatter == "Intrinsic" or scatter == "intrinsic":
            print("Warning - Kormondy and Ho's intrinsic scatter is effectively fixed, with a scale of 0.5")
            scatter = np.random.normal(0, 0.5, len(stellar_mass))
            log_black_hole_mass += scatter
        elif isinstance(type(scatter), float):
            scatter = np.random.normal(0, scatter, len(stellar_mass))
            log_black_hole_mass += scatter
        elif scatter == False or scatter == None:
            pass
        else:
            assert False, "Unknown Scatter argument {}".format(scatter)

    elif method == 'Eq4':
        log_black_hole_mass = 8.35 + 1.31 * (stellar_mass - 11)
        if scatter == "Intrinsic" or scatter == "intrinsic":
            print("Warning - Eq4's intrinsic scatter is effectively fixed, with a scale of 0.5")
            scatter = np.random.normal(0, 0.5, len(stellar_mass))
            log_black_hole_mass += scatter
        elif scatter == "fixed":
            scatter = np.random.normal(0, scatter, len(stellar_mass))
            log_black_hole_mass += scatter
        elif scatter == False or scatter == None:
            pass
        else:
            assert False, "Unknown Scatter argument {}".format(scatter)
    else:
        assert False, "Unknown method when assigning black hole mass - {}".format(method)

    if visual_debugging:
        width = 0.1
        bins = np.arange(6, 10, width)

        hist = np.histogram(log_black_hole_mass, bins=bins)[0]
        black_hole_mf = (hist/debugging_volume)/width
        log_black_hole_mf = np.log10(black_hole_mf[black_hole_mf != 0])
        adj_bins = bins[0:-1][black_hole_mf != 0]

        plt.figure()
        plt.loglog()
        plt.plot(10**adj_bins, (10**log_black_hole_mf), label="{}".format(method))
        plt.xlabel("Black Hole Mass")
        plt.ylabel("phi")
        plt.title("Black Hole Mass Function")
        plt.legend()
        save_path = visual_debugging_path + "Black_Hole_Mass_Function.png"
        print("Writing file: {}".format(save_path))
        plt.savefig(save_path)
        plt.close()

    return log_black_hole_mass


def to_duty_cycle(method, stellar_mass, black_hole_mass, z=0, data_path="./Data/DutyCycles/", suppress_output=False):
    """ Function to assign duty cycle.

    :param method: string/float. If string, should be a method (currently "Man16" or "Schulze"), if float will be value.
    :param stellar_mass: array, the stellar masses in log10.
    :param black_hole_mass: array, the black hole masses in log10
    :param z: float, redshift
    :param data_path: string, path to the directory where the data is stored.
    :return: array, the duty cycle
    """

    method_type = type(method)
    if method_type is float or method_type is int:
        duty_cycle = np.ones_like(stellar_mass) * method
    elif isinstance(method, str):
        if method == "Man16":
            if z > 0.1 and not supress_output:
                print("Warning - Mann's duty cycle is not set up for redshifts other than zero")
            mann_path = data_path + "Mann.csv"
            df = pd.read_csv(mann_path, header=None)
            mann_stellar_mass = df[0].values
            mann_duty_cycle = df[1].values
            get_u = sp.interpolate.interp1d(mann_stellar_mass, mann_duty_cycle, bounds_error=False,
                                            fill_value=(mann_duty_cycle[0], mann_duty_cycle[-1]))
            duty_cycle = get_u(stellar_mass)

        elif method == "Schulze":
            # Find the nearest file to the redshift we want
            schulze_path = data_path + utl.GetCorrectFile("Schulze", z, data_path)
            if not suppress_output:
                print("GetCorrectFile:", utl.GetCorrectFile("Schulze", z, data_path))
            if not suppress_output:
                print("Found Schulze file:", schulze_path)
            df = pd.read_csv(schulze_path, header=None)
            schulze_black_hole_mass = df[0].values
            schulze_duty_cycle = df[1].values
            duty_cycle = np.zeros_like(black_hole_mass)
            get_u = sp.interpolate.interp1d(schulze_black_hole_mass, schulze_duty_cycle, bounds_error=False,
                                            fill_value=(schulze_duty_cycle[-1], schulze_duty_cycle[0]))
            duty_cycle = 10 ** get_u(black_hole_mass)
            if not suppress_output:
                print(duty_cycle)
        elif method == "Geo":
            geo_stellar_mass, geo_duty_cycle = utl.ReadSimpleFile("Geo17DC", z, data_path)
            get_u = sp.interpolate.interp1d(geo_stellar_mass, geo_duty_cycle, bounds_error=False,
                                            fill_value=(geo_duty_cycle[0], geo_duty_cycle[-1]))

            duty_cycle = 10 ** get_u(stellar_mass)
        else:
            assert False, "Unknown Duty Cycle Type {}".format(method)
    else:
        assert False, "No duty cycle type specified"

  
    assert len(duty_cycle[(duty_cycle < 0) * (duty_cycle > 1)]) == 0, \
        "{} Duty Cycle elements outside of the range 0-1 exist. This is a probability, so this is not valid. Values: {}"\
        .format(len(duty_cycle[(duty_cycle < 0) * (duty_cycle > 1)]), duty_cycle[(duty_cycle < 0) * (duty_cycle > 1)])

    return duty_cycle


def edd_schechter_function(edd, method="Schechter", arg1=-1, arg2=-0.65, redshift_evolution=False, z=0, data_path="./Data/"):
    gammaz = 3.47
    gammaE = arg2
    z0 = 0.6
    A = 10. ** (-1.41)
    prob = ((edd / (10. ** arg1)) ** gammaE)

    if redshift_evolution:
        prob *= ((1. + z) / (1. + z0)) ** gammaz

    if method == "Schechter":
        return prob * np.exp(-(edd / (10. ** arg1)))
    elif method == "PowerLaw":
        return prob
    elif method == "Gaussian":
        return np.exp(-(edd - arg2) ** 2. / 2.*arg1 ** 2.)
    elif method == "Geo":
        geo_ed, geo_phi_top, geo_phi_bottom, z_new = utl.ReadSimpleFile("Geo17", z, data_path, cols=3, retz=True)
        mean_phi = (geo_phi_top + geo_phi_bottom)/2
        get_phi = sp.interpolate.interp1d(geo_ed, mean_phi, bounds_error=False, fill_value=(mean_phi[0], mean_phi[-1]))
        return get_phi(edd)
    else:
        assert False, "Type is unknown"



def black_hole_mass_to_luminosity(black_hole_mass,
                                  duty_cycle,
                                  stellar_mass,
                                  z=0,
                                  method="Schechter",
                                  redshift_evolution=False,
                                  parameter1=-1,
                                  parameter2=-0.65,
                                  return_plotting_data=False,
                                  volume=500**3):
    """ Function to assign the eddington ratios from black hole mass.

    :param black_hole_mass: array, the black hole mass (log10)
    :param duty_cycle: array, the duty cycle (only used for weightings in plotting data)
    :param stellar_mass: array, the stellar mass (log10)
    :param z: float, redshift
    :param method: string, the function to pull the Eddington Ratios from. Options are "Schechter", "PowerLaw" or
     "Gaussian".
    :param redshift_evolution: bool, if set to true will introduce a factor representing the z-evolution.
    :param parameter1: the first parameter for the method. For Schechter it is the knee, for PowerLaw it is not used,
    and for the Gaussian it is sigma.
    :param parameter2: the second parameter for the method. For Schechter it is alpha, for PowerLaw it is not used, for
    Gaussian it is b.
    :param return_plotting_data: bool, flag to return the plotting data for the eddington ratio distribution and the XLF
    :param volume: the cosmological volume, used for creating the plotting data.
    :return: luminosity (array), and if return_plotting_data is True, the XLF plotting data and eddington ratio
    distribution, both as PlottingData objects (see ACTUtillity).
    """

    l_edd = 38.1072 + black_hole_mass

    edd_bin = np.arange(-4, 0, 0.001)
    prob_schechter_function = edd_schechter_function(10 ** edd_bin, method=method, arg1=parameter1, arg2=parameter2,
                                                     redshift_evolution=redshift_evolution, z=z)
    p = prob_schechter_function * (10**0.0001)
    r_prob = p[::-1]
    prob_cum = np.cumsum(r_prob)
    r_prob_cum = prob_cum[::-1]
    y = r_prob_cum / r_prob_cum[0]
    y = y[::-1]
    edd_bin = edd_bin[::-1]

    a = np.random.random(len(black_hole_mass))
    y2edd_bin = sp.interpolate.interp1d(y, edd_bin, bounds_error=False, fill_value=(edd_bin[0], edd_bin[-1]))
    lg_edd = y2edd_bin(a)  # lgedd = np.interp(a, y, edd_bin)  # , right=-99)
    l_bol = lg_edd + l_edd
    lg_l_bol = l_bol - 33.49
    lg_lum = lg_l_bol - 1.54 - (0.24 * (lg_l_bol - 12.)) - \
        (0.012 * ((lg_l_bol - 12.) ** 2.)) + (0.0015 * ((lg_l_bol - 12.) ** 3.))
    luminosity = lg_lum + 33.49

    if not return_plotting_data:
        return luminosity

    # Save Luminosity Function Data
    step = 0.1
    bins = np.arange(42, 46, step)
    lum_bins = sp.stats.binned_statistic(luminosity, duty_cycle, 'sum', bins=bins)[0]
    lum_func = (lum_bins/volume)/step

    xlf_plotting_data = utl.PlottingData(bins[0:-1][lum_func > 0], np.log10(lum_func[lum_func > 0]))

    # Save Eddington Distribution Data
    step = 0.5
    lg_edd_derived = np.log10(25) + luminosity - (35.3802 + stellar_mass - 0.15)  # log10(1.26e38 * 0.002) = 35.3802
    edd_bin = np.arange(-4, 1, step)
    prob_derived = stats.binned_statistic(lg_edd_derived, duty_cycle, 'sum', bins=edd_bin)[0] / (
                step * sum(duty_cycle))

    edd_bin = edd_bin[:-1]
    edd_bin = edd_bin[prob_derived > 0]
    prob_derived = prob_derived[prob_derived > 0]

    edd_plotting_data = (utl.PlottingData(edd_bin, np.log10(prob_derived)))

    return luminosity, xlf_plotting_data, edd_plotting_data


def generate_nh_distribution(lg_luminosity, z, lg_nh):
    """ Function written by Viola to generate (I think) a distribution of nh values for the appropriate luminosity.

    :param lg_luminosity: float, value for luminosity (log10)
    :param z: float, redshift
    :param lg_nh: float, array of possible nh values
    :return: array, the probability distribution over lg_nh.
    """
    xi_min, xi_max, xi0_4375, a1, eps, fctk, beta = 0.2, 0.84, 0.43, 0.48, 1.7, 1., 0.24
    if z > 2:
        z = 2
    xi_4375 = (xi0_4375*(1+z)**a1) - beta*(lg_luminosity - 43.75)
    max_ = max(xi_4375, xi_min)
    xi = min(xi_max, max_)
    fra = (1 + eps)/(3 + eps)
    f = np.ones(len(lg_nh))

    # Boolean flags to separate conditions
    flag = np.where((lg_nh < 21) & (lg_nh >= 20))
    flag1 = np.where((lg_nh < 22) & (lg_nh >= 21))
    flag2 = np.where((lg_nh < 23) & (lg_nh >= 22))
    flag3 = np.where((lg_nh < 24) & (lg_nh >= 23))
    flag4 = np.where((lg_nh < 26) & (lg_nh >= 24))
    if xi < fra:
        f[flag] = 1 - ((2 + eps)/(1 + eps))*xi
        f[flag1] = (1/(1 + eps))*xi
        f[flag2] = (1/(1 + eps))*xi
        f[flag3] = (eps/(1 + eps))*xi
        f[flag4] = (fctk/2)*xi
    else:
        f[flag] = (2/3) - ((3 + 2*eps)/(3 + 3*eps))*xi
        f[flag1] = (1/3) - (eps/(3 + 3*eps))*xi
        f[flag2] = (1/(1 + eps))*xi
        f[flag3] = (eps/(1 + eps))*xi
        f[flag4] = (fctk/2)*xi
    return f


def generate_nh_value_robust(index, length, nh_bins, lg_lx_bins, z):
    nh_distribution = (generate_nh_distribution(lg_lx_bins[index], z, nh_bins)) * 0.01  # call fn
    cum_nh_distribution = np.cumsum(nh_distribution[::-1])[::-1]
    norm_cum_nh_distribution = (cum_nh_distribution/cum_nh_distribution[0])[::-1]
    reverse_nh_bins = nh_bins[::-1]  # Reverse
    sample = np.random.random(length)
    interpolator = sp.interpolate.interp1d(norm_cum_nh_distribution, reverse_nh_bins, bounds_error=False,
                                           fill_value=(reverse_nh_bins[0], reverse_nh_bins[-1]))
    return interpolator(sample)


def batch_nh(indexes, values, nh_bins, lg_lx_bins, z):
    out = []
    for index in indexes:
        flag = np.where(index == values)[0]
        if len(flag) == 0:
            pass
        else:
            component = (flag, generate_nh_value_robust(index, len(values[flag]),  nh_bins, lg_lx_bins, z))
            out.append(component)
    return out


def luminosity_to_nh(luminosity, z, parallel=True):
    """ function to generate nh values for the AGN based on the luminosity.

    :param luminosity: array, the luminosity of the AGNs (log10)
    :param z: float, redshift
    :return: array, the nh value(s)
    """

    lg_nh_range = np.arange(20., 30., 0.0001)  # Range, or effective 'possible values' of Nh
    nh = np.ones(len(luminosity))
    lg_lx_bins = np.arange(np.amin(luminosity), np.amax(luminosity), 0.01)
    bin_indexes = np.digitize(luminosity, lg_lx_bins)

    def generate_nh_value(index, length, nh_bins=lg_nh_range):
        nh_distribution = (generate_nh_distribution(lg_lx_bins[index], z, nh_bins)) * 0.01  # call fn

        cum_nh_distribution = np.cumsum(nh_distribution[::-1])[::-1]
        norm_cum_nh_distribution = (cum_nh_distribution/cum_nh_distribution[0])[::-1]
        
        reverse_nh_bins = nh_bins[::-1]  # Reverse
        sample = np.random.random(length)
        
        interpolator = sp.interpolate.interp1d(norm_cum_nh_distribution, reverse_nh_bins, bounds_error=False,
                                               fill_value=(reverse_nh_bins[0], reverse_nh_bins[-1]))

        return interpolator(sample)

    if not parallel:
        # This loop has been sped up but is still a bottleneck
        for i in range(len(lg_lx_bins) - 1):  # index for Lx bins
            # Serial
            flag = np.where(bin_indexes == i)
            nh[flag] = generate_nh_value(i, len(nh[flag]))

    if parallel:
        no_proc = multiprocessing.cpu_count()
        pool = multiprocessing.Pool(no_proc)
        indexes_list = np.array_split(np.arange(len(lg_lx_bins) - 1), no_proc)

        res = [pool.apply_async(batch_nh, (indexes, bin_indexes, lg_nh_range, lg_lx_bins, z)) for indexes in indexes_list]

        results = [r.get() for r in res]

        pool.close()
        pool.join()

        continuous_results = []
        for result in results:
            continuous_results += result
        for element in continuous_results:
                nh[element[0]] = element[1]

    return nh

def nh_to_type(nh):
    """ Simple function that takes an nh value and assigns the AGN type.

    :param nh: array, the nh value
    :return: array, the type
    """
    type1 = np.ones_like(nh)
    type2 = np.ones_like(nh) * 2
    thick = np.ones_like(nh) * 3

    type = np.zeros_like(nh)
    type[nh < 22] = type1[nh < 22]
    type[(nh >= 22) * (nh < 24)] = type2[[(nh >= 22) * (nh < 24)]]
    type[nh >= 24] = thick[nh >= 24]
    return type


def compute_wp(x, y, z, period, weights=None, bins=(-1, 1.5, 50), pimax=50, threads="system"):
    """ Function to encapsulate wp from Corrfunc.

    :param x: array, x coordinate
    :param y: array, y coordinate
    :param z: array, z coordinate
    :param period: float, the axis of the cosmological volume
    :param weights: array, weights (if any)
    :param bins: tuple, of form (low, high, number of steps), representing the bins for WP. Values are in log10.
    :param pi_max: float, the value of pi_max
    :param threads: int/string, the number of threads to spawn. Setting threads="system" will spawn a number of threads
    equal to the (available) cores on your machine. Exceeding this will not result in performance increase.
    :return: PlottingData object, with x as the bins and y as xi.
    """
    if threads == "System" or threads == "system":
        threads = multiprocessing.cpu_count()
    r_bins = np.logspace(bins[0], bins[1], bins[2])

    print("Weights, max = {}. , min = {}".format(np.amax(weights), np.amin(weights)))


    wp_results = wp(period, pimax, threads, r_bins, x, y, z, weights=weights, weight_type='pair_product', verbose=True)
    xi = wp_results['wp']
    return utl.PlottingData(r_bins[:-1], xi)


def compute_bias(variable, parent_halo_mass, z, h, cosmology, bin_size = 0.3, weight=None, mask=None):
    """ Function to compute the bias for a supplied variable. Viola wrote much of this function.

    :param variable: array, the variable to compute the bias against (log10)
    :param parent_halo_mass: array, the parent halo mass
    :param z: float, redshift
    :param h: float, reduced hubble constant
    :param cosmology: Corrfunc cosmology object, storing all the cosmological parameters.
    :param bin_size: float, size of the bins (log10) - bin high and low values are automatically calculated.
    :param weight: array, weights to the bias
    :param mask: array, if desired, we can mask the data
    :return: PlottingData object, with the bias vs the bins.
    """
    def func_g_squared(z):
        matter = cosmology.Om0 * (1 + z) ** 3
        curvature = (1 - cosmology.Om0 - cosmology.Ode0) * (1 + z) ** 2
        return matter + curvature + cosmology.Ode0

    def func_d(omega, omega_l):
        # Helper functions
        a = 5 * omega / 2
        b = omega ** (4 / 7.) - omega_l
        c = 1 + omega / 2
        d = 1 + omega_l / 70.
        d = a / (b + c * d)
        return d

    def func_omega_l(omega_l0, g_squared):
        # Eisenstein 1999
        return omega_l0 / g_squared

    def func_delta_c(delta_0_critical, d):
        # delta_c as a function of omega and linear growth
        # van den Bosch 2001
        return delta_0_critical/d

    def func_delta_0_critical(omega, p):
        # A3 van den Bosch 2001
        return 0.15 * (12 * 3.14159) ** (2 / 3.) * omega ** p

    def func_p(omega_m0, omega_l0):
        # A4 van den Bosch 2001
        if omega_m0 < 1 and omega_l0 == 0:
            return 0.0185
        if omega_m0 + omega_l0 == 1.0:
            return 0.0055
        else:
            return 0.0055  # VIOLA, had to add this in to make it work with my cosmology, I assume this is okay?

    def func_omega(z, omega_m0, g_squared):
        # A5 van den Bosch 2001 / eq 10 Eisenstein 1999
        return omega_m0 * (1 + z) ** 3 / g_squared

    def func_sigma(sigma_8, f, f_8):
        # A8 van den Bosch 2001
        return sigma_8 * f / f_8

    def func_u_8(gamma):
        # A9 van den Bosch 2001
        return 32 * gamma

    def func_u(gamma, M):
        # A9 van den Bosch 2001
        return 3.804e-4 * gamma * (M / cosmology.Om0) ** (1 / 3.)

    def func_f(u):
        # A10 van den Bosch 2001
        common = 64.087
        factors = (1, 1.074, -1.581, 0.954, -0.185)
        exps = (0, 0.3, 0.4, 0.5, 0.6)

        ret_val = 0.0
        for i in range(len(factors)):
            ret_val += factors[i] * u ** exps[i]

        return common * ret_val ** (-10)

    def func_b_eul(nu, delta_sc=1.686, a=0.707, b=0.5, c=0.6):
        # eq. 8 Sheth 2001
        a = np.sqrt(a) * delta_sc
        b = np.sqrt(a) * a * nu ** 2
        c = np.sqrt(a) * b * (a * nu ** 2) ** (1 - c)
        d = (a * nu ** 2) ** c
        e = (a * nu ** 2) ** c + b * (1 - c) * (1 - c / 2)
        return 1 + (b + c - d / e) / a

    def func_b_eul_tin(nu, delta_sc=1.686, a=0.707, b=0.35, c=0.8):
        # eq. 8 Tinker 2005
        a = np.sqrt(a) * delta_sc
        b = np.sqrt(a) * a * nu ** 2
        c = np.sqrt(a) * b * (a * nu ** 2) ** (1 - c)
        d = (a * nu ** 2) ** c
        e = (a * nu ** 2) ** c + b * (1 - c) * (1 - c / 2)
        return 1 + (b + c - d / e) / a

    def estimate_sigma(M, z, g_squared, omega_m0=cosmology.Om0, gamma=0.2, sigma_8=0.8):
        # Estimate sigma for a set of masses
        # vdb A9
        u = func_u(gamma, M)
        u_8 = func_u_8(gamma)
        # vdb A10
        f = func_f(u)
        f_8 = func_f(u_8)
        # vdb A8
        sigma = func_sigma(sigma_8, f, f_8)
        return sigma

    def estimate_delta_c(M, z, g_squared, gamma=0.2, omega_m0=cosmology.Om0, omega_L0=cosmology.Ode0):
        # Estimate delta_c for a set of masses
        # Redshift/model dependant parameters
        omega = func_omega(z, omega_m0, g_squared)
        omega_l = func_omega_l(omega_L0, g_squared)
        # vdb A3
        p = func_p(omega_m0, omega_L0)
        # Allevato code
        d1 = func_d(omega, omega_l) / (1 + z)
        d0 = func_d(omega_m0, omega_L0)
        d = d1 / d0
        delta_0_crit = func_delta_0_critical(omega, p)
        delta_0_crit = 1.686
        delta_c = func_delta_c(delta_0_crit, d)
        return delta_c, delta_0_crit

    def estimate_bias_tin(m, z, g_squared, gamma=0.2, omega_m0=cosmology.Om0, omega_L0=cosmology.Ode0,
                          sigma_8=0.8):
        # Estimate the bias Tinker + 2005
        sigma = estimate_sigma(m, z, g_squared, omega_m0, gamma, sigma_8)
        delta_c, delta_0_crit = estimate_delta_c(m, z, g_squared, gamma, omega_m0, omega_L0)
        nu = delta_c / sigma
        return func_b_eul_tin(nu, delta_0_crit)

    g_squared = func_g_squared(z)
    bias = estimate_bias_tin(((10**parent_halo_mass) * 0.974) * h, z, g_squared)
    bins = np.arange(np.amin(variable), np.amax(variable), bin_size)
    mean_bias = np.zeros_like(bins)
    error_bias = np.zeros_like(bins)

    for i in range(len(bins) - 1):
        n1 = np.where(((variable) >= bins[i]) & (variable < bins[i + 1]))
        if mask is not None:
            n1 *= mask  # Mask out for obscured etc if we want to.
        if weight is not None:
            mean_bias[i] = np.sum(bias[n1] * weight[n1]) / np.sum(weight[n1])
            error_bias[i] = np.sqrt((np.sum(weight[n1] * (bias[n1] - mean_bias[i]) ** 2)) / (
                        ((len(weight[n1]) - 1) / len(weight[n1])) * np.sum(weight[n1])))
        else:
            mean_bias[i] = np.mean(bias[n1])
            error_bias[i] = np.std(bias[n1])
    return utl.PlottingData(bins[0:-1], mean_bias[0:-1], error_bias[0:-1])


def calculate_hod(up_id, halo_mass, duty_cycle_weight, centrals=True):
    """ Function to estimate the HOD of a catalogue, only centrals.

    :param up_id: array, the up_id of a cataloguea
    :param halo_mass: array, the halo mass (log10)
    :param duty_cycle_weight: array, the duty cycle (for weighting)
    :param centrals: bool, flag to turn off only calculating for centrals, to calculate for all galaxies.
    :return:
    """
    flagCentrals = np.where(up_id > 0)  # Centrals

    #Halo_mass = np.log10(self.mvir)

    bins = np.arange(11, 15, 0.1)
    bins_out = bins[0:-1]

    hist_centrals_unweighted = np.histogram(halo_mass[flagCentrals], bins)[0]
    hist_all = np.histogram(halo_mass, bins)[0]

    if centrals:
        flag = flagCentrals
    elif not centrals:
        flag = np.invert(flagCentrals)
    else:
        assert False, "Invalid Value for centrals, should be Boolean"

    if duty_cycle_weight is not None:
        hist_subject = np.histogram(halo_mass[flag], bins, weights=duty_cycle_weight[flag])[0]
    else:
        hist_subject = np.histogram(halo_mass[flag], bins)[0]

    t = np.where(bins_out <= 11.7)  # Not quite sure what these are, or what they are for?
    h = np.where(bins_out > 11.2)
    l = np.where(bins_out <= 11.2)

    hod = np.zeros_like(bins_out)

    if not centrals:  # Not sure why we are doing this?
        hod[t] = 0.0001
        hod = hist_subject / hist_centrals_unweighted
    else:
        hod[h] = hist_subject[h] / hist_centrals_unweighted[h]
        hod[l] = hist_subject[l] / hist_all[l]

    return utl.PlottingData(bins[0:-1], hod)


if __name__ == "__main__":
    cosmo = 'planck18'
    cosmology = cosmology.setCosmology(cosmo)
    volume = 200**3

    halos = generate_semi_analytic_halo_catalogue(volume, (12, 16, 0.1), 0, 0.7,
                                                  visual_debugging=False,
                                                  erase_debugging_folder=True,
                                                  visual_debugging_path="./visualValidation/SemiAnalyticCatalog/")
    stellar_mass = halo_mass_to_stellar_mass(halos, 0,
                                             visual_debugging=False,
                                             erase_debugging_folder=True,
                                             debugging_volume=volume,
                                             visual_debugging_path="./visualValidation/StellarMass/")

    black_hole_mass = stellar_mass_to_black_hole_mass(stellar_mass,
                                                      method="Shankar16",
                                                      scatter="Intrinsic",
                                                      visual_debugging=False,
                                                      erase_debugging_folder=True,
                                                      debugging_volume=volume,
                                                      visual_debugging_path="./visualValidation/BlackHoleMass/")

    duty_cycle = to_duty_cycle("Geo", stellar_mass, black_hole_mass, 0)

    luminosity = black_hole_mass_to_luminosity(black_hole_mass, duty_cycle, stellar_mass, 0)

    nh = luminosity_to_nh(luminosity, 0)
    agn_type = nh_to_type(nh)


